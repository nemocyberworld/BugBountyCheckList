<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Content Discovery Script (Single Target)</title>
<style>
  body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 900px; margin: 2em auto; padding: 0 1em; }
  h1, h2, h3 { color: #2a7ae2; }
  code { background: #f4f4f4; padding: 0.2em 0.4em; border-radius: 3px; }
  pre { background: #f9f9f9; border-left: 4px solid #2a7ae2; padding: 1em; overflow-x: auto; white-space: pre-wrap; }
  ul { margin-top: 0; }
  .tool { margin-bottom: 1.5em; }
  button {
    cursor: pointer;
    margin-top: 10px;
    background-color: #2a7ae2;
    border: none;
    color: white;
    padding: 8px 14px;
    border-radius: 5px;
    font-size: 1em;
  }
  .note { color:#555; font-size:.95em }
</style>
</head>
<body>

<h1>üìö Content Discovery Script (robots.txt &amp; sitemap.xml) ‚Äî Single Target</h1>

<p>This Python script automates content discovery on a <strong>single target</strong> by fetching and parsing <code>robots.txt</code> and <code>sitemap.xml</code>, checking URL statuses, and saving results for recon.</p>

<hr />

<div class="tool">
  <h2>Python Script: Content Discovery (robots.txt &amp; sitemap.xml)</h2>
  <button onclick="toggleScript(event)" aria-expanded="false" aria-controls="pythonScript">
    ‚ñ∂ Show Python Script
  </button>
  <pre id="pythonScript" style="display:none;">
#!/usr/bin/env python3
# Single-target content discovery for robots.txt & sitemap.xml

import requests, sys
from urllib.parse import urljoin
from bs4 import BeautifulSoup

def normalize_domain(domain: str) -> str:
    d = domain.strip()
    if not d.startswith(("http://", "https://")):
        d = "https://" + d
    return d.rstrip("/")

def fetch_robots_txt(base):
    url = urljoin(base, "/robots.txt")
    print(f"\n[*] Fetching robots.txt from {url}")
    try:
        r = requests.get(url, timeout=12)
        if r.status_code != 200:
            print(f"[-] robots.txt not found (status {r.status_code})")
            return []
        paths = []
        for line in r.text.splitlines():
            line = line.split("#", 1)[0].strip()
            if line.lower().startswith("disallow:"):
                _, val = line.split(":", 1)
                path = val.strip()
                if path and path != "/":
                    paths.append(path)
        return paths
    except Exception as e:
        print(f"[-] Error fetching robots.txt: {e}")
        return []

def fetch_sitemap_urls(base):
    candidates = ["/sitemap.xml", "/sitemap_index.xml"]
    urls = set()
    for cand in candidates:
        url = urljoin(base, cand)
        print(f"[*] Trying sitemap at {url}")
        try:
            r = requests.get(url, timeout=12)
            if r.status_code != 200:
                print(f"[-] Not found (status {r.status_code}) at {url}")
                continue
            soup = BeautifulSoup(r.text, "xml")
            locs = [loc.text.strip() for loc in soup.find_all("loc")]
            for loc in locs:
                urls.add(loc)
        except Exception as e:
            print(f"[-] Error fetching {url}: {e}")
    return sorted(urls)

def check_url_status(u):
    try:
        r = requests.head(u, timeout=10, allow_redirects=True)
        return r.status_code
    except Exception:
        try:
            r = requests.get(u, timeout=12, allow_redirects=True)
            return r.status_code
        except Exception:
            return "Error"

def main():
    print("\n=== üîç Content Discovery (robots.txt & sitemap.xml) ‚Äî Single Target ===")
    if len(sys.argv) > 1:
        domain = sys.argv[1]
    else:
        domain = input("\nüåê Enter the target (e.g., example.com or https://example.com): ").strip()
    base = normalize_domain(domain)

    discovered = []

    # robots.txt
    robots_paths = fetch_robots_txt(base)
    if robots_paths:
        print(f"\n[+] Found {len(robots_paths)} disallowed paths:")
        for p in robots_paths:
            full = urljoin(base, p)
            status = check_url_status(full)
            print(f"  [ {status} ] {full}")
            discovered.append((full, status))
    else:
        print("[-] No disallowed paths in robots.txt.")

    # sitemap(s)
    sm_urls = fetch_sitemap_urls(base)
    if sm_urls:
        print(f"\n[+] Found {len(sm_urls)} URLs in sitemap(s):")
        for u in sm_urls:
            status = check_url_status(u)
            print(f"  [ {status} ] {u}")
            discovered.append((u, status))
    else:
        print("[-] No URLs found in sitemap(s).")

    # save results
    if discovered:
        with open("discovered_content.txt", "w") as f:
            for u, s in discovered:
                f.write(f"{s} {u}\n")
        print("\nüìÅ Results saved to discovered_content.txt\n")
    else:
        print("\n[!] No URLs discovered to save.\n")

if __name__ == "__main__":
    main()
  </pre>
</div>

<script>
function toggleScript(event) {
  const scriptBlock = document.getElementById('pythonScript');
  const btn = event.target;
  const isHidden = scriptBlock.style.display === 'none' || scriptBlock.style.display === '';
  scriptBlock.style.display = isHidden ? 'block' : 'none';
  btn.textContent = isHidden ? '‚ñº Hide Python Script' : '‚ñ∂ Show Python Script';
  btn.setAttribute('aria-expanded', String(isHidden));
}
</script>

<hr />

<h2>Summary</h2>
<ul>
  <li>Fetch &amp; parse <code>robots.txt</code> (disallowed paths often hint at sensitive areas).</li>
  <li>Parse <code>sitemap.xml</code>/<code>sitemap_index.xml</code> to enumerate URLs.</li>
  <li>Verify with HTTP status codes; persist to <code>discovered_content.txt</code>.</li>
  <li>Use results to seed directory bruteforcers and crawlers for deeper discovery.</li>
</ul>

<hr />

<h2>Tools in Kali Linux &amp; BlackArch for Content Discovery (Single Target)</h2>
<p class="note">These are widely used for robots/sitemap parsing and broader content discovery. Replace <code>&lt;target&gt;</code> with a host or base URL. Availability can vary by image/version.</p>

<h3>Kali Linux</h3>
<ul>
  <li><strong>curl</strong> ‚Äî Quick fetch of robots/sitemaps.<br><code>curl -s https://&lt;target&gt;/robots.txt</code> &nbsp; <code>curl -s https://&lt;target&gt;/sitemap.xml</code></li>
  <li><strong>httpx</strong> ‚Äî Probe URL lists for status/tech.<br><code>httpx -l urls.txt -mc 200,301,302 -o live.txt</code></li>
  <li><strong>gobuster (dir)</strong> ‚Äî Wordlist discovery to expand from robots hints.<br><code>gobuster dir -u https://&lt;target&gt; -w wordlist.txt -x php,txt,bak</code></li>
  <li><strong>ffuf</strong> ‚Äî Fast fuzzer for paths/params (seed with sitemap).<br><code>ffuf -u https://&lt;target&gt;/FUZZ -w wordlist.txt -mc 200,204,301,302,403</code></li>
  <li><strong>dirsearch</strong> ‚Äî Recursive content discovery.<br><code>dirsearch -u https://&lt;target&gt; -w wordlist.txt -e php,txt,bak -o out.txt</code></li>
  <li><strong>feroxbuster</strong> ‚Äî Rust-based fast directory/file discovery.<br><code>feroxbuster -u https://&lt;target&gt; -w wordlist.txt -r</code></li>
  <li><strong>dirb</strong> ‚Äî Classic dir brute-force with bundled lists.<br><code>dirb https://&lt;target&gt; /usr/share/dirb/wordlists/common.txt</code></li>
  <li><strong>katana</strong> ‚Äî High-speed crawler to enrich URLs beyond sitemap.<br><code>katana -u https://&lt;target&gt; -o katana.txt</code></li>
  <li><strong>hakrawler</strong> ‚Äî Lightweight crawler to pull paths from HTML/JS.<br><code>echo https://&lt;target&gt; | hakrawler -d 2 -u &gt; hakrawler.txt</code></li>
  <li><strong>waybackurls</strong> ‚Äî Archived URLs to complement sitemap results.<br><code>echo &lt;target&gt; | waybackurls &gt; wayback.txt</code></li>
  <li><strong>gau</strong> ‚Äî Pull URLs from Wayback/CC/URLScan for one domain.<br><code>echo &lt;target&gt; | gau &gt; gau.txt</code></li>
  <li><strong>unfurl</strong> ‚Äî Extract paths/params from URL lists.<br><code>cat urls.txt | unfurl -u paths &gt; paths.txt</code></li>
  <li><strong>uro</strong> ‚Äî Normalize/dedupe noisy URL lists.<br><code>cat urls.txt | uro &gt; urls_clean.txt</code></li>
</ul>

<h3>BlackArch</h3>
<ul>
  <li><strong>curl</strong> ‚Äî Fetch robots/sitemaps quickly.<br><code>curl -s https://&lt;target&gt;/robots.txt</code> &nbsp; <code>curl -s https://&lt;target&gt;/sitemap.xml</code></li>
  <li><strong>httpx</strong> ‚Äî Liveness and status probing at scale.<br><code>httpx -l urls.txt -mc 200,301,302 -o live.txt</code></li>
  <li><strong>gobuster</strong> ‚Äî Directory discovery with extensions.<br><code>gobuster dir -u https://&lt;target&gt; -w wordlist.txt -x php,txt</code></li>
  <li><strong>ffuf</strong> ‚Äî Rapid path/param fuzzing.<br><code>ffuf -u https://&lt;target&gt;/FUZZ -w wordlist.txt</code></li>
  <li><strong>dirsearch</strong> ‚Äî Python-based recursive discovery.<br><code>dirsearch -u https://&lt;target&gt; -w wordlist.txt -o out.txt</code></li>
  <li><strong>feroxbuster</strong> ‚Äî Fast Rust-based recursion.<br><code>feroxbuster -u https://&lt;target&gt; -w wordlist.txt -r</code></li>
  <li><strong>dirb</strong> ‚Äî Legacy but useful baseline scans.<br><code>dirb https://&lt;target&gt; /usr/share/dirb/wordlists/common.txt</code></li>
  <li><strong>katana</strong> ‚Äî Speedy crawler for target URL discovery.<br><code>katana -u https://&lt;target&gt; -o katana.txt</code></li>
  <li><strong>hakrawler</strong> ‚Äî Crawl to uncover links/assets quickly.<br><code>echo https://&lt;target&gt; | hakrawler -d 2 -u &gt; hakrawler.txt</code></li>
  <li><strong>waybackurls</strong> ‚Äî Get archived URLs for a domain.<br><code>echo &lt;target&gt; | waybackurls &gt; wayback.txt</code></li>
  <li><strong>gau</strong> ‚Äî Multi-source archive URL gatherer.<br><code>echo &lt;target&gt; | gau &gt; gau.txt</code></li>
  <li><strong>unfurl</strong> ‚Äî Parse URL components at scale.<br><code>cat urls.txt | unfurl -u keys &gt; keys.txt</code></li>
  <li><strong>uro</strong> ‚Äî Clean/dedupe URL lists for scanning.<br><code>cat urls.txt | uro &gt; urls_clean.txt</code></li>
</ul>

<p class="note">Tip: Use <code>robots.txt</code> disallowed paths as seeds for <em>ffuf/feroxbuster/dirsearch</em>. Use <em>sitemap.xml</em> to prioritize likely valid content first.</p>

<hr />

<div style="margin-top: 2em;">
  <a href="javascript:history.back()" style="text-decoration: none; font-weight: bold; font-size: 1.1em;">
    &#8592; Go Back
  </a>
</div>

</body>
</html>
