<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>JavaScript Recon & Secrets Analysis (Single Target)</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 2em auto;
      padding: 1em;
      background-color: #f9f9f9;
      color: #222;
      max-width: 1000px;
    }
    h1, h2, h3 { color: #0070c9; }
    summary {
      font-weight: bold;
      cursor: pointer;
      background: #dbeeff;
      padding: 0.6em;
      border-radius: 5px;
      margin-bottom: 0.5em;
    }
    details[open] summary {
      border-bottom: 1px solid #c3dbff;
      border-radius: 5px 5px 0 0;
    }
    pre {
      background: #f4f4f4;
      padding: 1em;
      border-left: 4px solid #0070c9;
      overflow-x: auto;
    }
    iframe {
      width: 100%;
      height: 100vh;
      border: none;
      border-radius: 6px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
      margin-top: 1em;
    }
    ul { padding-left: 20px; }
    hr { border: none; height: 1px; background: #ccc; margin: 2em 0; }
    .muted { color:#555; font-size:.95em }
    .grid { display:grid; gap:1rem }
    @media(min-width:720px){ .grid{ grid-template-columns:1fr 1fr } }
    code.kbd { background:#eef6ff; border:1px solid #cde3ff; padding:.1em .35em; border-radius:4px }
  </style>
</head>
<body>

<h1>ğŸ§  JavaScript Recon & Secrets Analysis â€” Single Target</h1>
<p>This page automates <strong>single-domain</strong> JavaScript reconnaissance (collect JS, mine endpoints, detect secrets) and embeds a GUI analyzer.</p>

<hr />

<h2>ğŸ”§ Single-Target Automation Bash Script</h2>
<details>
  <summary>ğŸ“œ Click to Expand Script</summary>
  <pre>
#!/bin/bash
# ğŸ” JavaScript Recon & Secrets Extractor (Single Target)
# Deps: waybackurls, curl, linkfinder, SecretFinder, js-beautify, httpx (optional)

set -euo pipefail
echo -e "\n\033[1;36mğŸš€ JavaScript Analysis Automation Script (Single Domain)\033[0m"
read -rp $'\033[1;33mEnter target domain (e.g., example.com): \033[0m' TARGET

if [[ -z "${TARGET}" ]]; then
  echo -e "\033[1;31m[!] No domain entered. Exiting.\033[0m"; exit 1
fi

mkdir -p results/"${TARGET}"/{js-files,extracted/endpoints,extracted/secrets,beautified,lists}

echo -e "\n\033[1;34m[1] ğŸ” Harvesting JS URLs from archives...\033[0m"
waybackurls "${TARGET}" | grep -E '\.js(\?|$)' | sort -u > results/"${TARGET}"/lists/js-urls.txt
echo -e "\033[1;32m[+] Found $(wc -l < results/"${TARGET}"/lists/js-urls.txt) JS URLs.\033[0m"

echo -e "\n\033[1;34m[2] ğŸŒ (Optional) Check which JS are live...\033[0m"
if command -v httpx >/dev/null 2>&1; then
  httpx -l results/"${TARGET}"/lists/js-urls.txt -mc 200 -silent \
    > results/"${TARGET}"/lists/js-urls-live.txt
  SRC="results/${TARGET}/lists/js-urls-live.txt"
  echo -e "\033[1;32m[+] Live JS: $(wc -l < "$SRC")\033[0m"
else
  SRC="results/${TARGET}"/lists/js-urls.txt
fi

echo -e "\n\033[1;34m[3] ğŸ“¥ Downloading JS files...\033[0m"
cd results/"${TARGET}"/js-files
while read -r url; do
  fname=$(echo "$url" | sed 's/[^a-zA-Z0-9]/_/g')
  echo -e "\033[1;32m[+] $url\033[0m"
  curl -sL --max-time 20 "$url" -o "${fname}.js" || true
done < "../lists/$(basename "$SRC")"
cd - >/dev/null

echo -e "\n\033[1;34m[4] ğŸ”— Extracting endpoints with LinkFinder...\033[0m"
for f in results/"${TARGET}"/js-files/*.js; do
  [[ -e "$f" ]] || continue
  name=$(basename "$f")
  linkfinder -i "$f" -o cli > results/"${TARGET}"/extracted/endpoints/"$name".endpoints.txt || true
done

echo -e "\n\033[1;34m[5] ğŸ”‘ Extracting secrets with SecretFinder...\033[0m"
for f in results/"${TARGET}"/js-files/*.js; do
  [[ -e "$f" ]] || continue
  name=$(basename "$f")
  secretfinder -i "$f" -o cli > results/"${TARGET}"/extracted/secrets/"$name".secrets.txt || true
done

echo -e "\n\033[1;34m[6] ğŸ¨ Beautifying JS with js-beautify (if installed)...\033[0m"
if command -v js-beautify >/dev/null 2>&1; then
  for f in results/"${TARGET}"/js-files/*.js; do
    [[ -e "$f" ]] || continue
    name=$(basename "$f")
    js-beautify "$f" > results/"${TARGET}"/beautified/"$name".pretty.js || true
  done
else
  echo -e "\033[1;33m[~] js-beautify not found; skipping prettification.\033[0m"
fi

echo -e "\n\033[1;32mâœ… Done!\033[0m"
echo -e "  ğŸ“ results/${TARGET}/js-files             (downloaded JS)"
echo -e "  ğŸ”— results/${TARGET}/extracted/endpoints  (extracted endpoints)"
echo -e "  ğŸ” results/${TARGET}/extracted/secrets    (extracted secrets)"
echo -e "  ğŸ¨ results/${TARGET}/beautified           (pretty JS)"
echo -e "  ğŸ—‚  results/${TARGET}/lists                (URL lists)"
  </pre>
</details>

<hr />

<h2>ğŸ§ª Embedded Full JS Analyzer Tool</h2>
<p>Use the GUI to search variables, endpoints, keys, and suspicious patterns inside JS quickly.</p>
<iframe src="https://nemocyberworld.github.io/Content-Analyzer/" title="Universal Content Analyzer"></iframe>

<hr />

<h2>ğŸ“¦ Script Output Structure</h2>
<ul>
  <li><strong>ğŸ“ JS Files:</strong> <code>./results/&lt;domain&gt;/js-files/</code></li>
  <li><strong>ğŸ“„ Endpoints:</strong> <code>./results/&lt;domain&gt;/extracted/endpoints/</code></li>
  <li><strong>ğŸ” Secrets:</strong> <code>./results/&lt;domain&gt;/extracted/secrets/</code></li>
  <li><strong>ğŸ¨ Beautified:</strong> <code>./results/&lt;domain&gt;/beautified/</code></li>
  <li><strong>ğŸ—‚ Lists:</strong> <code>./results/&lt;domain&gt;/lists/</code></li>
</ul>

<hr />

<h2>ğŸ§° Tools in Kali Linux &amp; BlackArch for JS Recon/Secrets (Single Target)</h2>
<p class="muted">Replace <code class="kbd">&lt;domain&gt;</code> with your target. If a tool isn't present, install via <code>apt</code> (Kali) or <code>pacman</code>/<code>blackman</code> (BlackArch), or use release binaries.</p>

<div class="grid">
  <div>
    <h3>Kali Linux (commonly available)</h3>
    <ul>
      <li><strong>waybackurls</strong> â€” Pull archived URLs for one domain.<br><code>echo &lt;domain&gt; | waybackurls &gt; wayback.txt</code></li>
      <li><strong>gau</strong> â€” Aggregate URLs from Wayback/CC/URLScan.<br><code>echo &lt;domain&gt; | gau &gt; gau.txt</code></li>
      <li><strong>waymore</strong> â€” Advanced archive recon + filters/dedup.<br><code>waymore -i "&lt;domain&gt;" -mode U -oU waymore.txt</code></li>
      <li><strong>waybackpy</strong> â€” Wayback Python CLI for single domain pulls.<br><code>waybackpy --url https://&lt;domain&gt;/ --unique &gt; waybackpy.txt</code></li>
      <li><strong>hakrawler</strong> â€” Crawl live site to enrich endpoints.<br><code>echo "https://&lt;domain&gt;" | hakrawler -d 3 -u &gt; hakrawler.txt</code></li>
      <li><strong>katana</strong> â€” High-speed crawler (seed lists for scans).<br><code>katana -u https://&lt;domain&gt; -o katana.txt</code></li>
      <li><strong>LinkFinder</strong> â€” Extract endpoints from JS.<br><code>linkfinder -i file.js -o cli</code></li>
      <li><strong>SecretFinder</strong> â€” Grep-like detector for API keys/tokens in JS/HTML.<br><code>secretfinder -i file.js -o cli</code></li>
      <li><strong>trufflehog</strong> â€” High-entropy &amp; rule-based secret finder (repos/dirs).<br><code>trufflehog filesystem ./results/&lt;domain&gt;/js-files</code></li>
      <li><strong>gitleaks</strong> â€” Fast secret scanning with rulesets (good on dumps).<br><code>gitleaks detect -s ./results/&lt;domain&gt;/js-files -r gitleaks.json</code></li>
      <li><strong>ripgrep (rg)</strong> â€” Ultra-fast regex search for patterns in JS.<br><code>rg -n --hidden -e '(api[_-]?key|secret|token)' ./results/&lt;domain&gt;/js-files</code></li>
      <li><strong>js-beautify</strong> â€” Beautify/minified JS for readability.<br><code>js-beautify file.min.js &gt; pretty.js</code></li>
      <li><strong>subjs</strong> â€” Grab JS file URLs from web pages.<br><code>echo https://&lt;domain&gt; | subjs &gt; subjs.txt</code></li>
      <li><strong>getJS</strong> â€” Enumerate JS links from pages and sitemaps.<br><code>getJS --url https://&lt;domain&gt; --output getjs.txt</code></li>
      <li><strong>xnLinkFinder</strong> â€” Advanced JS endpoint extractor (fork).</strong><br><code>python3 xnLinkFinder.py -i file.js -o out.txt</code></li>
      <li><strong>httpx</strong> â€” Probe URLs for liveness/status/tech.<br><code>httpx -l wayback.txt -mc 200,301,302 -o live.txt</code></li>
      <li><strong>unfurl</strong> â€” Extract paths/params/keys from URL lists.<br><code>cat wayback.txt | unfurl -u keys &gt; keys.txt</code></li>
      <li><strong>uro</strong> â€” Normalize &amp; dedupe noisy URL lists.<br><code>cat wayback.txt | uro &gt; clean.txt</code></li>
      <li><strong>arjun</strong> â€” Discover hidden parameters on endpoints.<br><code>arjun -u https://&lt;domain&gt;/path</code></li>
    </ul>
  </div>

  <div>
    <h3>BlackArch (scanner/recon categories)</h3>
    <ul>
      <li><strong>waybackurls</strong> â€” Wayback URLs for a domain.<br><code>echo &lt;domain&gt; | waybackurls &gt; wayback.txt</code></li>
      <li><strong>gau</strong> â€” Multi-source archived URL puller.<br><code>echo &lt;domain&gt; | gau &gt; gau.txt</code></li>
      <li><strong>waymore</strong> â€” Archive recon + filters/dedup.<br><code>waymore -i "&lt;domain&gt;" -mode U -oU waymore.txt</code></li>
      <li><strong>waybackpy</strong> â€” Python Wayback client.<br><code>waybackpy --url https://&lt;domain&gt;/ --unique &gt; waybackpy.txt</code></li>
      <li><strong>hakrawler</strong> â€” Lightweight crawler for endpoints.<br><code>echo "https://&lt;domain&gt;" | hakrawler -d 3 -u &gt; hakrawler.txt</code></li>
      <li><strong>katana</strong> â€” High-speed crawler for target URLs.<br><code>katana -u https://&lt;domain&gt; -o katana.txt</code></li>
      <li><strong>LinkFinder</strong> â€” Endpoint extraction from JS files.<br><code>linkfinder -i file.js -o cli</code></li>
      <li><strong>SecretFinder</strong> â€” Detect secrets in JS/HTML content.<br><code>secretfinder -i file.js -o cli</code></li>
      <li><strong>trufflehog</strong> â€” Advanced secret scanning (entropy + rules).<br><code>trufflehog filesystem ./results/&lt;domain&gt;/js-files</code></li>
      <li><strong>gitleaks</strong> â€” Rule-based secret detector (fast CI usage).<br><code>gitleaks detect -s ./results/&lt;domain&gt;/js-files</code></li>
      <li><strong>ripgrep (rg)</strong> â€” Fast regex sweeping of JS directories.<br><code>rg -n '(aws_access_key_id|authorization|bearer)' ./results/&lt;domain&gt;/js-files</code></li>
      <li><strong>js-beautify</strong> â€” Make minified JS readable for review.<br><code>js-beautify file.min.js &gt; pretty.js</code></li>
      <li><strong>subjs</strong> â€” Extract JS URLs from web responses.<br><code>echo https://&lt;domain&gt; | subjs &gt; subjs.txt</code></li>
      <li><strong>getJS</strong> â€” Discover JS from pages/sitemaps/robots.<br><code>getJS --url https://&lt;domain&gt; --output getjs.txt</code></li>
      <li><strong>xnLinkFinder</strong> â€” Enhanced JS endpoint finder.<br><code>python3 xnLinkFinder.py -i file.js -o out.txt</code></li>
      <li><strong>httpx</strong> â€” Validate archived URLs are live.<br><code>httpx -l wayback.txt -o live.txt -mc 200,301,302</code></li>
      <li><strong>unfurl</strong> â€” Pull components (paths/params/keys) from URLs.<br><code>cat wayback.txt | unfurl -u keys &gt; keys.txt</code></li>
      <li><strong>uro</strong> â€” Clean/normalize URL sets.<br><code>cat wayback.txt | uro &gt; clean.txt</code></li>
      <li><strong>arjun</strong> â€” Hidden parameter discovery on endpoints.<br><code>arjun -u https://&lt;domain&gt;/path</code></li>
    </ul>
  </div>
</div>

<p class="muted">âš ï¸ Tool availability can vary by image/version. Treat any credential/secret findings responsibly and with permission.</p>

<hr />

<div style="margin-top: 2em;">
  <a href="javascript:history.back()" style="text-decoration: none; font-weight: bold; font-size: 1.1em;">
    &#8592; Go Back
  </a>
</div>

</body>
</html>
